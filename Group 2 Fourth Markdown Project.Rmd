---
title: "Group 2 Fourth Markdown Project"
author: "Raphael Lee, Javier Bolong, Allen Abel, Tricia Pulmano"
date: "`r Sys.Date()`"
output:
  tufte::tufte_html: default
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
link-citations: yes
---

```{r setup, include=FALSE}
library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```
# Introduction


# Methodology



# Simple Linear Regression

<br>

> ### A. Geting the Linear Regression

## Graphs
But first, we must graph the values to see the if there is a relationship between them.

Graphs are shown below through the use of R.
```{r table, echo=FALSE}
library(readxl)
bullying <- read_excel("~/GitHub/Group-2-Fourth-Markdown-Project/Table.xlsx")
Fun <- bullying$`Being made fun of`
Rumor <- bullying$`Spreading rumors`
Grade <- bullying$`Grade`
```

``` {r mlgraphs, echo = FALSE}
plot(bullying)
```

``` {r funvrumor, echo=FALSE}
plot(Fun, Rumor, main = "Plot of the types of bullying")
abline(lm(Rumor~Fun), col = "red")
```

``` {r gradevfun, echo=FALSE}
plot(Grade, Fun)
abline(lm(Fun~Grade), col = "red")
```

``` {r gradevrumor, echo=FALSE}
plot(Grade, Rumor)
abline(lm(Rumor~Grade), col = "red")
```


Let's display the values imported from the excel table:


``` {r kable1, echo=FALSE}
library(kableExtra)
kable(bullying)
```

The values above show the amount of people bullied per grade level as well as the type of bullying they experience

We use these graphs to examine whether there is a relationship between the variables. These graphs will also help us in forming a linear regression model.


# Multiple Linear Regression

We can use the Formula for linear regression model:
\[
  Y=\beta_0+\beta_1x_1+\beta_2x_2+\epsilon
\]



Before we can use that equation, we need to use the least square equation to get the value of $\hat{\beta_0},\hat{\beta_1}$ and $\hat{\beta_2}$ which is represented by:



$$n{\hat{\beta}}_0 + {\hat{\beta}}_1\sum_{i=1}^{n}x_{i1}+ {\hat{\beta}}_2\sum_{i=1}^{n}x_{i2}= \sum_{i=1}^{n}y_{i}$$
$${\hat{\beta}}_0\sum_{i=1}^{n}x_{i1} + {\hat{\beta}}_1\sum_{i=1}^{n}x_{i1}^2+ {\hat{\beta}}_2\sum_{i=1}^{n}x_{i1}x_{i2}= \sum_{i=1}^{n}x_{i1}y_{i}$$

$${\hat{\beta}}_0\sum_{i=1}^{n}x_{i2} + {\hat{\beta}}_1\sum_{i=1}^{n}x_{i1}x_{i2}+ {\hat{\beta}}_2\sum_{i=1}^{n}x_{i2}^2 = \sum_{i=1}^{n}x_{i2}y_{i}$$



We can just use R to solve for this


``` {r summary1}
summaries1 <- lm(Grade ~ Fun + Rumor)
summary(summaries1)
confint(summaries1)
```
Using R, we see that the values of $\hat{\beta_0},\hat{\beta_1}$ and $\hat{\beta_2}$ which are:
$\hat{\beta_0}=1.164e+01$,

$\hat{\beta_1}=-5.469e-05$,

$\hat{\beta_2}=3.066e-05$

Finally after substituting, the linear regression equation would:
\begin{align}
&y=(1.164e+01)+(-5.469e-05)(x_1)-(3.066e-05)(x_2)
\end{align}

<br>
<br>



> ### B. Estimating σ2.

<br>
To estimate $σ^2$, we can use the following formula:
\[
  σ^2=\frac{SS_E}{n-p}
\]

But we could just use R again to calculate it for use. Thus using the following code:

``` {r}
summaries2 <- lm(Grade ~ Fun + Rumor)
anova(summaries2)
```

Looking at the summary we could see that $σ^2$ is estimated to be 1.5505.

<br>
<br>

> ### C. Computing for the standard errors of the regression coefficients.

<br>
To compute for the standard errors of the regression coefficients we can use the equation:
\[
  se(\beta_j)=\sqrt{\sigma^2C_{jj}}\\
\]
Where $C_{jj}=(X'X)^{-1}.$

Using the data from the table we can construct $X$ and $X'$ which is represented by these table respectively:

``` {r X and X, include=TRUE, echo=FALSE}
xvalues <- c(1,134604,111331,1,175819,121939,1,90390,75904,1,94074,79775,1,100143,102528,1,51431,59613,1,29275,50996) 
x <- matrix(xvalues,nrow=7,ncol=3,byrow=TRUE)
x
t(x) 
xtrans <- t(x)
```


Then we can compute for $X'X$

``` {r XX, include=TRUE, echo=FALSE}
x <- matrix(xvalues,nrow=7,ncol=3,byrow=TRUE)
xtrans <- t(x)
xx <- (xtrans %*% x)
xx
```

With $X'X$ computed we can now look for its inverse $(X'X)^{-1}$:

``` {r inverse, include=TRUE, echo=FALSE}
x <- matrix(xvalues,nrow=7,ncol=3,byrow=TRUE)
xxinv <- solve(xx)
xxinv
```

The variances of the regression coefficients are given by the diagonal elements thus  $C_{00}$ is 4.371973e+00, $C_{11}$ is 8.405655e-10, and $C_{22}$ is 2.825788e-09 and now we can solve $se(\beta_0)$,$se(\beta_1)$ and $se(\beta_2)$:
\[
\begin{align} 
&se(\beta_0)=\sqrt{0.3721(4.371973e+00)}=1.275465\\
&se(\beta_1)=\sqrt{0.3721(8.405655e-10)}=1.768543e-05\\
&se(\beta_2)=\sqrt{0.3721(2.825788e-09)}=3.242647e-05\\
\end{align}
\]

Equations are shown below:

``` {r equations}
b0 <- sqrt(0.3721*4.371973e+00)
b0
b1 <- sqrt(0.3721*8.405655e-10)
b1
b2 <- sqrt(0.3721*2.825788e-09)
b2
```


We can also see these numbers in the summary table we did earlier

``` {r}
summaries1 <- lm(Grade ~ Fun + Rumor)
summary(summaries1)
```

<br>
<br>


The Hypothesis for these are:

$H_0$:$\beta_1=\beta_2=0$


$H_1$:$\beta_j≠0$ for at least one j

While the equation need is :

$F_0=\frac{\frac{SS_r}{k}}{\frac{SS_E}{n-p}}=\frac{MS_r}{MS_e}$

To Find $SS_R$, we must first find $SS_T$ as $SS_R=SS_T-SS_E$

We can use this equation to fine SS_T:

\[
  SS_T=y'y-(\frac{(\sum{y_I}1)^2}{n})    
\]

we can use R to find $y'y$
``` {r YY, include=TRUE, echo=FALSE}
library(knitr)
y.data <- c(6,7,8,9,10,11,12) 
y <- matrix(y.data,nrow=7,ncol=1,byrow=TRUE)
yp <- t(y)
ypy <- (yp %*% y)
ypy
```
$(\sum{y_I}1)^2)$ can be solve:
$(\sum{y_I}1)^2=(6+7+8+9+10+11+12)^2=3969$

Substituting the value we can now solve for $SS_T$ as well and $SS_R$
\[
\begin{align} 
  SS_T&=595-(\frac{(3969)^2}{7})\\
  &=32446
  \end{align} 
\]
Given that $SS_E$ is equal to 1.4886 (See Summaries2 )
\[
\begin{align} 
  SS_R&=32446-7927.6\\
  &=24518
  \end{align} 
\]

Finally we can get the value of $F_0$

$F_0=\frac{\frac{SS_r}{k}}{\frac{SS_E}{n-p}}=\frac{MS_r}{MS_e}$
\[
\begin{align} 
  \end{align} 
\]

# Discussion

# Conclusion

